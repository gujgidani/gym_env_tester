{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:56:37.390430Z",
     "start_time": "2024-07-16T20:56:35.954820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import gym_envs.envs.traffic_light_support_functions as tlsf\n",
    "import traci\n",
    "\n",
    "cycleTime = 60\n",
    "minGreentime = 5\n",
    "numberOfPhases = 6\n",
    "intergreenMatrix = np.zeros((numberOfPhases, numberOfPhases))\n",
    "intergreenMatrix = [\n",
    "    [0, 0, 0, 7, 6, 6],\n",
    "    [0, 0, 0, 5, 0, 0],\n",
    "    [8, 0, 0, 0, 5, 0],\n",
    "    [6, 6, 0, 0, 8, 0],\n",
    "    [8, 0, 6, 5, 0, 0],\n",
    "    [6, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "phasePlan = np.full((numberOfPhases,cycleTime), 0)\n",
    "\n",
    "initialPhaseplan = np.full((numberOfPhases, cycleTime),'r')\n",
    "\n",
    "starting_phases = [1, 1, 2, 2, 1, 2]\n",
    "phase_lengths = [\n",
    "    [19, 2, 14, 3, 22],\n",
    "    [41, 2, 13, 4],\n",
    "    [2, 34, 3, 21],\n",
    "    [2, 13, 3, 42],\n",
    "    [40, 2, 14, 3, 1],\n",
    "    [2, 13, 3, 42]\n",
    "]\n",
    "\n",
    "initialPhaseplan = tlsf.generate_phase_plan(starting_phases, phase_lengths)\n",
    "initialPhaseplan = tlsf.change_phase_plan((5,5),60)"
   ],
   "id": "274c4fdd4613b346",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:56:42.875619Z",
     "start_time": "2024-07-16T20:56:37.391623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#env = gym.make('TrafficEnv-V0', render_mode ='console', starting_phases = starting_phases, phase_lengths = phase_lengths, simulation_time= 3600*5,phase_change_step=5)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    #\"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ],
   "id": "fc28ba58794d70ad",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T20:56:45.226990Z",
     "start_time": "2024-07-16T20:56:42.876276Z"
    }
   },
   "source": [
    "import torch\n",
    "import time\n",
    "from torchrl.envs import GymEnv, StepCounter, TransformedEnv, GymWrapper\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "gym_env = GymWrapper(gym.make('TrafficEnv-V0', render_mode ='console', starting_phases = starting_phases, phase_lengths = phase_lengths, simulation_time= 3600*5,phase_change_step=5),categorical_action_encoding=True)\n",
    "env = TransformedEnv(gym_env, device=device)\n",
    "env.set_seed(0)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/data/replay_buffers/samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "795726461"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:56:45.231390Z",
     "start_time": "2024-07-16T20:56:45.227686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
    "\n",
    "value_mlp = MLP(out_features=env.action_spec.n, num_cells=[128, 128])\n",
    "value_net = Mod(value_mlp, in_keys=[\"occupancy\",'vehicle_count'], out_keys=[\"action_value\"])\n",
    "policy = Seq(value_net, QValueModule(spec=env.action_spec))\n",
    "exploration_module = EGreedyModule(\n",
    "    env.action_spec, annealing_num_steps=10_000, eps_init=0.9\n",
    ")\n",
    "policy_explore = Seq(policy, exploration_module)"
   ],
   "id": "3f2f7a30f39b7e35",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:56:45.334804Z",
     "start_time": "2024-07-16T20:56:45.232557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "\n",
    "init_rand_steps = 0\n",
    "frames_per_batch = 100\n",
    "optim_steps = 10\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=-1,\n",
    ")\n",
    "rb = ReplayBuffer(storage=LazyTensorStorage(100_000))\n",
    "\n",
    "from torch.optim import Adam"
   ],
   "id": "d9881219166632e8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:56:49.713183Z",
     "start_time": "2024-07-16T20:56:45.335575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "\n",
    "loss = DQNLoss(value_network=policy, action_space=env.action_spec, delay_value=True)\n",
    "optim = Adam(loss.parameters(), lr=0.02)\n",
    "updater = SoftUpdate(loss, eps=0.99)"
   ],
   "id": "2db10adf1fd7da8f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:57:06.233310Z",
     "start_time": "2024-07-16T20:56:49.714701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.record import WandbLogger, VideoRecorder\n",
    "\n",
    "path = \"./training_loop\"\n",
    "#logger = CSVLogger(exp_name=\"dqn\", log_dir=path)\n",
    "logger = WandbLogger(exp_name=\"dqn\")"
   ],
   "id": "e6b70950bd1f048c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mwagnertamas\u001B[0m (\u001B[33mwagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/wagnertamas/Documents/Munka/SUMO networks/gym_env_tester/wandb/run-20240716_225656-0n76cpwa</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gym_env_tester/runs/0n76cpwa' target=\"_blank\">dqn</a></strong> to <a href='https://wandb.ai/wagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gym_env_tester' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/wagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gym_env_tester' target=\"_blank\">https://wandb.ai/wagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gym_env_tester</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/wagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gym_env_tester/runs/0n76cpwa' target=\"_blank\">https://wandb.ai/wagnertamas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gym_env_tester/runs/0n76cpwa</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:57:07.240190Z",
     "start_time": "2024-07-16T20:57:06.234889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_count = 0\n",
    "total_episodes = 0\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(collector):\n",
    "    # Write data in replay buffer\n",
    "    rb.extend(data)\n",
    "    max_length = rb[:][\"next\", \"step_count\"].max()\n",
    "    if len(rb) > init_rand_steps:\n",
    "        # Optim loop (we do several optim steps\n",
    "        # per batch collected for efficiency)\n",
    "        for _ in range(optim_steps):\n",
    "            sample = rb.sample(128)\n",
    "            loss_vals = loss(sample)\n",
    "            loss_vals[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # Update exploration factor\n",
    "            exploration_module.step(data.numel())\n",
    "            # Update target params\n",
    "            updater.step()\n",
    "            if i % 10:\n",
    "                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n",
    "                #Log the results\n",
    "            logger.log_scalar(\"loss\", loss_vals[\"loss\"].item(), total_count)\n",
    "            logger.log_scalar(\"epsilon\", exploration_module.eps, total_count)\n",
    "            logger.log_scalar(\"max_steps\", max_length, total_count)\n",
    "            #logger.log_scalar(\"reward\", data[\"reward\"].mean().item(), total_count)\n",
    "            logger.log_scalar(\"steps\", data[\"next\", \"done\"].sum().item(), total_count)\n",
    "            logger.log_scalar(\"total_episodes\", total_episodes, total_count)\n",
    "                \n",
    "            total_count += data.numel()\n",
    "            total_episodes += data[\"next\", \"done\"].sum()\n",
    "    if max_length > 200:\n",
    "        \n",
    "        break\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "torchrl_logger.info(\n",
    "    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n",
    ")\n"
   ],
   "id": "8c9c4d00e68ae86c",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m total_episodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      3\u001B[0m t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(collector):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# Write data in replay buffer\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     rb\u001B[38;5;241m.\u001B[39mextend(data)\n\u001B[1;32m      7\u001B[0m     max_length \u001B[38;5;241m=\u001B[39m rb[:][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_count\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmax()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/collectors/collectors.py:888\u001B[0m, in \u001B[0;36mSyncDataCollector.iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_frames \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_frames:\n\u001B[1;32m    887\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 888\u001B[0m     tensordict_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout()\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_frames \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tensordict_out\u001B[38;5;241m.\u001B[39mnumel()\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_frames \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m total_frames:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/_utils.py:480\u001B[0m, in \u001B[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _os_is_windows \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_distributed_rpc\u001B[38;5;241m.\u001B[39mPyRRef):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_value()\n\u001B[0;32m--> 480\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/collectors/collectors.py:1007\u001B[0m, in \u001B[0;36mSyncDataCollector.rollout\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1006\u001B[0m     env_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shuttle\n\u001B[0;32m-> 1007\u001B[0m env_output, env_next_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep_and_maybe_reset(env_input)\n\u001B[1;32m   1009\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shuttle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m env_output:\n\u001B[1;32m   1010\u001B[0m     \u001B[38;5;66;03m# ad-hoc update shuttle\u001B[39;00m\n\u001B[1;32m   1011\u001B[0m     next_data \u001B[38;5;241m=\u001B[39m env_output\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/envs/common.py:2746\u001B[0m, in \u001B[0;36mEnvBase.step_and_maybe_reset\u001B[0;34m(self, tensordict)\u001B[0m\n\u001B[1;32m   2704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_and_maybe_reset\u001B[39m(\n\u001B[1;32m   2705\u001B[0m     \u001B[38;5;28mself\u001B[39m, tensordict: TensorDictBase\n\u001B[1;32m   2706\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001B[1;32m   2707\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001B[39;00m\n\u001B[1;32m   2708\u001B[0m \n\u001B[1;32m   2709\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2744\u001B[0m \u001B[38;5;124;03m            is_shared=False)\u001B[39;00m\n\u001B[1;32m   2745\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2746\u001B[0m     tensordict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(tensordict)\n\u001B[1;32m   2747\u001B[0m     \u001B[38;5;66;03m# done and truncated are in done_keys\u001B[39;00m\n\u001B[1;32m   2748\u001B[0m     \u001B[38;5;66;03m# We read if any key is done.\u001B[39;00m\n\u001B[1;32m   2749\u001B[0m     tensordict_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_mdp(tensordict)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/envs/common.py:1461\u001B[0m, in \u001B[0;36mEnvBase.step\u001B[0;34m(self, tensordict)\u001B[0m\n\u001B[1;32m   1458\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assert_tensordict_shape(tensordict)\n\u001B[1;32m   1459\u001B[0m next_preset \u001B[38;5;241m=\u001B[39m tensordict\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1461\u001B[0m next_tensordict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step(tensordict)\n\u001B[1;32m   1462\u001B[0m next_tensordict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_proc_data(next_tensordict)\n\u001B[1;32m   1463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m next_preset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1464\u001B[0m     \u001B[38;5;66;03m# tensordict could already have a \"next\" key\u001B[39;00m\n\u001B[1;32m   1465\u001B[0m     \u001B[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001B[39;00m\n\u001B[1;32m   1466\u001B[0m     \u001B[38;5;66;03m# the necessary keys\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/envs/transforms/transforms.py:779\u001B[0m, in \u001B[0;36mTransformedEnv._step\u001B[0;34m(self, tensordict)\u001B[0m\n\u001B[1;32m    777\u001B[0m next_preset \u001B[38;5;241m=\u001B[39m tensordict\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    778\u001B[0m tensordict_in \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39minv(tensordict)\n\u001B[0;32m--> 779\u001B[0m next_tensordict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_env\u001B[38;5;241m.\u001B[39m_step(tensordict_in)\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m next_preset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    781\u001B[0m     \u001B[38;5;66;03m# tensordict could already have a \"next\" key\u001B[39;00m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001B[39;00m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;66;03m# the necessary keys\u001B[39;00m\n\u001B[1;32m    784\u001B[0m     next_tensordict\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m    785\u001B[0m         next_preset\u001B[38;5;241m.\u001B[39mexclude(\u001B[38;5;241m*\u001B[39mnext_tensordict\u001B[38;5;241m.\u001B[39mkeys(\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    786\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/torchrl/envs/gym_like.py:294\u001B[0m, in \u001B[0;36mGymLikeEnv._step\u001B[0;34m(self, tensordict)\u001B[0m\n\u001B[1;32m    285\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrapper_frame_skip):\n\u001B[1;32m    287\u001B[0m     (\n\u001B[1;32m    288\u001B[0m         obs,\n\u001B[1;32m    289\u001B[0m         _reward,\n\u001B[1;32m    290\u001B[0m         terminated,\n\u001B[1;32m    291\u001B[0m         truncated,\n\u001B[1;32m    292\u001B[0m         done,\n\u001B[1;32m    293\u001B[0m         info_dict,\n\u001B[0;32m--> 294\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_transform(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_env\u001B[38;5;241m.\u001B[39mstep(action_np))\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _reward \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    297\u001B[0m         reward \u001B[38;5;241m=\u001B[39m reward \u001B[38;5;241m+\u001B[39m _reward\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:49\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/sumo_RL_pytorch/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:208\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[0;34m(env, action)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001B[39;00m\n\u001B[0;32m--> 208\u001B[0m result \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    210\u001B[0m     result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[1;32m    211\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects step result to be a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/Munka/SUMO networks/gym_env_tester/gym_envs/envs/traffic_env.py:156\u001B[0m, in \u001B[0;36mTrafficEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphase_plan \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mroll(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphase_plan, \u001B[38;5;241m1\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    155\u001B[0m \u001B[38;5;66;03m#self.phase_plan[:, -1] = tlsf.change_phase_plan(self.actions[action])\u001B[39;00m\n\u001B[0;32m--> 156\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphase_plan[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m tlsf\u001B[38;5;241m.\u001B[39mchange_phase_plan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactions[action[\u001B[38;5;241m0\u001B[39m]], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcycle_time)\n\u001B[1;32m    157\u001B[0m travel_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    158\u001B[0m mean_speed \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
